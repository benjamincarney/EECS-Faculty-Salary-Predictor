{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "#Takes in csv file and loads it so that it is a pytorch tensor for training and testing data\n",
    "def loader(full=True, RMP=False):\n",
    "\tX_train, y_train, X_test, y_test = None, None, None, None\n",
    "\n",
    "\t#Read csv file and drop unncecessary columns\n",
    "\tdata = pd.read_csv('updated_years_of_employment.csv')\n",
    "\tdata = data.drop(['LAST','FIRST','UID','MIDDLE','APPT FRACTION','AMT OF SALARY PAID FROM GENL FUND',\n",
    "\t\t'FOS', 'Field14', 'url', 'X1', 'found'], axis=1)\n",
    "\tdata.replace([\"NaN\", 'N/A', '--', '', np.nan], 'NA', inplace = True)\n",
    "\t#Get number of unique values\n",
    "\tcolumns = list(data.columns.values)\n",
    "\tunique_vals_set = set()\n",
    "\tfor i in columns:\n",
    "\t\tif i != 'APPT ANNUAL FTR':\n",
    "\t\t\tunique_vals_set.update(set(data[i].unique().tolist()))\n",
    "\tunique_vals = len(unique_vals_set)\n",
    "\n",
    "\tunique_vals_list = list(unique_vals_set)\n",
    "\n",
    "\tindex = 1\n",
    "\tdictionary = {'NaN':0}\n",
    "\tfor index, val in enumerate(unique_vals_list):\n",
    "\t\tif val not in dictionary:\n",
    "\t\t\tdictionary[unique_vals_list[index]] = index\n",
    "\t\t\tindex += 1\n",
    "\n",
    "\tdata = data.replace(dictionary)\n",
    "\n",
    "\tif full:\n",
    "\t\t#Turn data to numpy array\n",
    "\t\tdata = data.values\n",
    "\t\t#Full dataset variables of X and Y\n",
    "\t\ty_vals = data[:, 17]\n",
    "\t\tX_vals = np.delete(data, 17, axis=1)\n",
    "\telif RMP:\n",
    "\t\tdata = data[['quality', 'difficulty', 'wtapercent', 'reviews', 'APPT ANNUAL FTR']]\n",
    "\t\t#Turn data to numpy array\n",
    "\t\tdata = data.values\n",
    "\t\t#Full dataset variables of X and Y\n",
    "\t\ty_vals = data[:, 4]\n",
    "\t\tX_vals = data[:, :4]\n",
    "\telse:\n",
    "\t\tdata = data[['APPOINTMENT TITLE', 'APPT FTR BASIS', 'citations', 'Service Dt', 'APPT ANNUAL FTR']]\n",
    "\t\t#Turn data to numpy array\n",
    "\t\tdata = data.values\n",
    "\t\t#Full dataset variables of X and Y\n",
    "\t\ty_vals = data[:, 4]\n",
    "\t\tX_vals = data[:, :4]\n",
    "\n",
    "\n",
    "\n",
    "\tindices = np.random.permutation(y_vals.shape[0])\n",
    "\ttraining_idx = indices[:math.floor(y_vals.shape[0] * 0.8)], \n",
    "\ttest_idx = indices[math.floor(y_vals.shape[0] * 0.8):math.floor(y_vals.shape[0] * 0.9)]\n",
    "\tval_idx = indices[math.floor(y_vals.shape[0] * 0.9):]\n",
    "\t\n",
    "\tX_train = X_vals[training_idx, :]\n",
    "\tX_test = X_vals[test_idx,:]\n",
    "\ty_train = y_vals[training_idx]\n",
    "\ty_test = y_vals[test_idx]\n",
    "\tX_validate = X_vals[val_idx, :]\n",
    "\ty_validate = y_vals[val_idx]\n",
    "\n",
    "\t#np.reshape(X_train, (173,1))\n",
    "\tX_train = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "\ty_mean = np.mean(y_train)\n",
    "\ty_sd = math.sqrt(np.var(y_train))\n",
    "\n",
    "\ty_train = (y_train - y_mean) / y_sd\n",
    "\ty_test = (y_test - y_mean) / y_sd\n",
    "\ty_validate = (y_validate - y_mean) / y_sd\n",
    "\n",
    "\treturn X_train, y_train, X_test, y_test, X_validate, y_validate, unique_vals\n",
    "\n",
    "\n",
    "#Our Model's class\n",
    "class NeuralNet(nn.Module):\n",
    "\tdef __init__(self, embed_units, hidden_units1=50, hidden_units2=100, output_units=1, inp_units=20):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t#Change these to our liking. Maybe add Batchnorm or L2 Normalization?\n",
    "\t\t#Also maybe do weight initialization ourselves?\n",
    "\n",
    "\t\tself.embed = nn.Embedding(embed_units, 20) #Figure out how we want to do embedding\n",
    "\t\tself.fc = nn.Sequential(\n",
    "\t\t\t# N x ? tensor (? WILL BE KNOWN ONCE EMBEDDING HAS BEEN IMPLEMENTED)\n",
    "\t\t\tnn.Linear(20, hidden_units1),\n",
    "\t\t\tnn.LeakyReLU(negative_slope=.5, inplace=True),\n",
    "\t\t\tnn.BatchNorm1d(18),\n",
    "\t\t\tnn.Dropout(0.2),\n",
    "\t\t\t# N x 100 tensor\n",
    "\t\t\tnn.Linear(hidden_units1, hidden_units2),\n",
    "\t\t\tnn.LeakyReLU(negative_slope=.5, inplace=True),\n",
    "\t\t\tnn.BatchNorm1d(18),\n",
    "\t\t\tnn.Dropout(0.1),\n",
    "\t\t\t# N x 200 tensor\n",
    "\t\t\tnn.Linear(hidden_units2, hidden_units1),\n",
    "\t\t\tnn.Tanh(),\n",
    "\t\t\t# N x 100 tensor\n",
    "\t\t\tnn.Linear(hidden_units1,output_units),\n",
    "\t\t\tnn.Tanh()\n",
    "\t\t\t# N x 1 tensor\n",
    "\t\t\t)\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# x is an N x dim tensor\n",
    "\t\ty_hat = self.embed(x.long()) #Add the embedding once figured out\n",
    "\t\ty_hat = self.fc(y_hat) \n",
    "\t\treturn y_hat\n",
    "\n",
    "#Set weights of model\n",
    "def init_weights(m):\n",
    "\tif type(m) == nn.Linear:\n",
    "\t\ttorch.nn.init.xavier_uniform(m.weight)\n",
    "\t\tm.bias.data.fill_(0.01)\n",
    "\n",
    "def NeuralTrain(trainloader, net, criterion, optimizer, device):\n",
    "\tloss_graph = []\n",
    "\tfor epoch in range(50):  # loop over the dataset for x number of epochs\n",
    "\t\tstart = time.time()\n",
    "\t\trunning_loss = 0.0\n",
    "\n",
    "\t\t#For each batch run through model, backprop, and optimize weights\n",
    "\t\tfor i, (data, salary) in enumerate(trainloader):\n",
    "\t\t\tdata = data.to(device).float()\n",
    "\t\t\tsalary = salary.to(device).float()\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutput = net(data)\n",
    "\t\t\tloss = criterion(output, salary)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# print statistics\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\t\t\n",
    "\t\t\tif i % 10 == 9:\n",
    "\t\t\t\tloss_graph.append(loss.item())\n",
    "\t\t\t\tend = time.time()\n",
    "\t\t\t\tprint('[epoch %d, iter %5d] loss: %.3f eplased time %.3f' % \n",
    "\t\t\t\t\t(epoch + 1, i + 1, running_loss / 100, end - start))\n",
    "\t\t\t\tstart = time.time()\n",
    "\t\t\t\trunning_loss = 0.0\n",
    "    \n",
    "\t# Plot learning curve\n",
    "\tfig1, ax1 = plt.subplots()\n",
    "\tax1.plot(loss_graph, '--')\n",
    "\tax1.set_title('Learning curve.')\n",
    "\tax1.set_ylabel('L1 Loss')\n",
    "\tax1.set_xlabel('Optimization steps.')\n",
    "\n",
    "\tprint('Finished Training')\n",
    "\n",
    "def NeuralTest(testloader, net, criterion, device):\n",
    "\ttotal = 0\n",
    "\terror = []\n",
    "\t#fig2, ax2 = plt.subplots()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor data in testloader:\n",
    "\t\t\trepresentations, salary = data\n",
    "\t\t\trepresentations = representations.to(device).float()\n",
    "\t\t\tsalary = salary.to(device).float()\n",
    "\t\t\toutputs = net(representations)\n",
    "\t\t\tloss = criterion(outputs, salary)\n",
    "\t\t\t'''\n",
    "\t\t\tax2.plot(outputs.numpy(), salary.numpy(), 'r+')\n",
    "\t\t\tax2.set_title('Prediction Plot')\n",
    "\t\t\tax2.set_ylabel('Actual Salary')\n",
    "\t\t\tax2.set_xlabel('Prediction')\n",
    "\t\t\t'''\n",
    "\t\t\terror.append(loss)\n",
    "\tprint('Error: %s dollars' % (np.mean(error)))\n",
    "\n",
    "def main():\n",
    "\t#Sets device to cpu or gpu if you have one\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\t#Get our datasets loaded\n",
    "\tX_train, y_train, X_test, y_test, X_validate, y_validate, unique_vals = loader(False, True)\n",
    "\n",
    "\t#Base model\n",
    "\tknn = KNeighborsRegressor(n_neighbors=5)\n",
    "\tknn.fit(X_train, y_train)\n",
    "\ty_test_pred = knn.predict(X_test)\n",
    "\ty_valid_pred = knn.predict(X_validate)\n",
    "\n",
    "\tprint(mean_absolute_error(y_test, y_test_pred))\n",
    "\tprint(mean_absolute_error(y_validate, y_valid_pred))\n",
    "\n",
    "\n",
    "\t#Turn numpy matrices into pytorch tensors for neural network\n",
    "\tX_train = torch.tensor(X_train.astype(dtype = 'float32'))\n",
    "\ty_train = torch.tensor(y_train.astype(dtype = 'float32'))\n",
    "\tX_test = torch.tensor(X_test.astype(dtype = 'float32'))\n",
    "\ty_test = torch.tensor(y_test.astype(dtype = 'float32'))\n",
    "\tX_validate = torch.tensor(X_validate.astype(dtype = 'float32'))\n",
    "\ty_validate = torch.tensor(y_validate.astype(dtype = 'float32'))\n",
    "\n",
    "\t#Put them into torch datasets with batch size \n",
    "\t#BATCH SIZE CAN CHANGE TO WHATEVER WORKS BEST\n",
    "\ttrainset = data_utils.TensorDataset(X_train, y_train)\n",
    "\ttrainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True)\n",
    "\n",
    "\ttestset = data_utils.TensorDataset(X_test, y_test)\n",
    "\ttestloader = torch.utils.data.DataLoader(testset, batch_size=y_test.shape[0], shuffle=False)\n",
    "\n",
    "\tvalidset = data_utils.TensorDataset(X_validate, y_validate)\n",
    "\tvalidloader = torch.utils.data.DataLoader(validset, batch_size=y_validate.shape[0], shuffle=False)\n",
    "\n",
    "\t#Model and Loss\n",
    "\tnet = NeuralNet(embed_units=unique_vals).to(device)\n",
    "\tnet.apply(init_weights)\n",
    "\tcriterion = nn.L1Loss()\n",
    "\n",
    "\t#Can also switch from adam to sgd if we so choose\n",
    "\toptimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=0.1)\n",
    "\n",
    "\t#Train, Test, and Validate model\n",
    "\tNeuralTrain(trainloader, net, criterion, optimizer, device)\n",
    "\tNeuralTest(testloader, net, criterion, device)\n",
    "\tNeuralTest(validloader, net, criterion, device)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
